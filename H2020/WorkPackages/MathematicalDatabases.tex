\TOWRITE{SL}{Proofread WP 6 Databases pass 1}
\TOWRITE{ALL}{Proofread WP 6 Databases pass 2}
\begin{draft}
\TOWRITE{MK (Work Package Lead)}{For WP leaders, please check the following (remove items
once completed)}, the
\begin{verbatim}
- [X] have all the tasks in this Work Package a lead institution?
- [X] have all deliverables in the WP a lead institution?
- [ ] do all tasks list all sites involved in them? 
- [ ] does the table of sites and their PM efforts match lists of sites for each task?
      (each site from the table is listed in all relevant tasks, and no site is listed
      only in the table or only at some task)
\end{verbatim}

\end{draft}

\def\DKS{\ensuremath{\mathcal{DKS}}\xspace}

\begin{workpackage}[id=dksbases,%wphases=1-48!.5,
  title=Data/Knowledge/Software-Bases,lead=JU,
  ZHRM=12,JURM=36,UWRM=25,SARM=10,LLRM=2,PSRM=4]

\begin{wpobjectives}
  The ultimate purpose of a mathematical VRE is to create \emph{data} ($\mathcal{D}$; see
  Section~\ref{sec:innovation}), knowledge ($\mathcal{K}$), and software ($\mathcal{S}$)
  by conducting modeling world situations, computing mathematical objects, and running
  computational experiments. To be effective a VRE needs an infrastructure that supports
  the creation, management, access, and dissemination of \DKS-strutures.  All
  the systems considered in this proposal (\GAP, \Sage, \Pari, \Singular, OEIS, arXiv.org,
  \ldots) already include data, knowledge, and software modules as part of their regular
  distribution, but not in a form that is interoperable between systems, severely limiting
  the usefulness of the systems and results. The objectives of this work package are
\begin{compactenum}
\item to design metadata- and representation formats for trans-system $\mathcal{DKS}$
  structures as a basis for a math VRE, 
\item implement interfaces to existing systems for interoperability and compatibility with
  the RE, and
\item implement a joint \DKS infrastructure for, searching, documentation, traceability,
  versioning, provenance, visualisation and native dissemination of \TheProject results
  (the latter three together with \WPref{UI}).
\end{compactenum}
Concretely we will design and build an infrastructure that would make it easy for either
individual mathematicians or a distributed collaboration to manage and use such
interlinked mathematical data. This work would provide part of the backend to \WPref{UI},
and would draw on previous work with the \LMFDB and \FindStat (which will be treated as
prototypes for our purposes, to serve as exemplars to other projects) and in return will
substantially enhance their capabilities.

User prerequisites should be kept to a minimum (depending on contributors' and users'
needs and goals), and in particular would not require any background in databases to
contribute new data or perform queries.
\end{wpobjectives}

\begin{wpdescription}
  We need ways to represent \DKS in the same representational system, make the \DKS
  structures explicit and therefore machine-manageable and -- since current
  computational/experimental mathematics involve quite extensive \DKS -- we need a new
  kind of ``database'', which we will call Mathematical Data/Knowledge/Software-base
  (\textbf{\DKS base}), and which we will build in this work package.

  The starting points for this unification effort will be the system-origented data bases
  for $\mathcal{D}$, the OMDoc (\underline{O}pen \underline{M}athematical
  \underline{Doc}uments) framework~\cite{Kohlhase:OMDoc1.2} for $\mathcal{K}$.
  OMDoc/MMT~\cite{RabKoh:WSMSML13} is a representation format for mathematical documents
  and knowledge that incorporates a metalogical framework to be foundation-independent,
  which allows interoperability between various ontolgoies/foundations of mathematics. For
  the integration of $\mathcal{K}$ and $\mathcal{S}$ we will build on the notion of
  \emph{biform theories} developed by Carette/Farmer~\cite{btc07} and extended to
  OMDoc/MMT by \site{JU} in~\cite{KohManRab:aumftg13}. In this setup, the programming
  language serves


 We will  The complexity is on vivid display in the \emph{L-functions and Modular Forms database}
  project (\LMFDB): while the general shape of the functional equation of an $L$-function
  is dependent on a lot of theoretical knowledge, it also requires parameter data and the
  coefficients of the associated Dirichlet series. Once this is obtained, highly optimised
  (and heavily parallelizable) algorithms can be run to compute values of this function.
\end{wpdescription}

\begin{tasklist}
\begin{task}[title={Survey of existing \DKS bases, Formulation of requirements},
  id=data-assessment,lead=ZH,partners={JU,SA,UW,US},wphases=0-3!.5,PM=2]
  In this task, we will survey existing databases, the technology used to implement them,
  how they were linked to the rest of the existing infrastructure and the functionalities
  offered. We will also select additional external data and projects to add to this
  effort, aiming to maximise the impact of our work.

  We will organise a workshop associated to this task (\localdelivref{wsrep})
\end{task}

\begin{task}[id=data-design,lead=JU,partners={ZH,US,SA,UW,LL},wphases=5-8,PM=2,
  title={and design of new  infrastructure when appropriate}]
  \TOWRITE{MK}{Rewrite database requirements design task -- done second pass,
    Michael have a look?}

  Ontologies are the canonical method used to implement databases that require significant
  data interchange. However, because of the extreme reification present in mathematics
  (relations between objects themselves become objects of study), there are specific
  obstacles compared to the usual semantic web model of publishing.

  Drawing on semantic web/Linked Open Data experience of the \site{LL} group, specialised to
  mathematics through the OMDoc/MMT work of the Bremen group, we will design a
  decentralised infrastructure for \TheProject. This infrastructure would allow modular
  collaborations, through decentralised hosting of data without the need to merge
  everything centrally.

\end{task}

\begin{task}[title=Triform Theories in OMDoc/MMT,id=data-triform,
  lead=JU,partners={ZH},PM=6,wphases=6-12]
  OMDoc/MMT is a representation language for mathematical knowledge and documents. work here would extend this along the data axis, which
  will require a specialised but integrated treatment.
\end{task}

\begin{task}[title=Computational Foundation for Python/Sage (or some CAS),
  id=data-foundationCAS,lead=JU,partners={ZH,SA},PM=6,wphases=12-18]

In the OMDoc/MMT world a foundation is a logical base language that
gives the formal meaning to all objects represented/formalized in
it. We have created a very initial computational foundation for Scala
and implemented it in the MMT API. This can be used to execute (or
verify) computations directly in OMDoc/MMT and thus forms the basis
for various integration tasks for OMDoc/MMT biform theories that
integrate Scala computations. Here we propose to develop a somewhat
more complete computational foundation for Python and/or parts of Sage
(coverage to be determined). Bi/Triform theories come in three parts:
\begin{compactitem}

\item syntax: what operators/types are there, how do they nest, 
\item computation:  what does the computation relation look like (sometimes called operational semantics). The declarative semantics of a computational foundation can be given as an OMDoc/MMT theory morphism into another foundation (e.g. a set theory);
\item \TOWRITE{MK}{there are supposed to be three parts, but i can t think of the third -- POD}
\end{compactitem}
\end{task}

\begin{task}[title=OEIS Case Study (Coverage and automated import),id=data-OEIS,lead=JU
,PM=6,wphases=12-18]
  In this case study we test the practical coverage of the trifunctional modules, by
  transforming an existing, high-profile database (the Online Sequence of Integer
  Sequences \url[http://www.oeis.org]) into OMDoc/MMT. The OEIS has about 250 thousand
  sequences, with formulae, descriptions, definitions, references, software, etc. in a
  structured text file (but no standardized format for formulae and references), so we
  expect to get 250 k theories. Having the OEIS in OMDoc/MMT form allows to do Knowledge
  Management services (presentation, definition lookup, formula search, ...) in
  \MathHub (see \WPref{UI}). The OEIS is a good case study, since the data is licensed
  under a Creative Commons license which allows derived works. The large size will allow statistically
  significant semantic cross-validation of the heuristic transformation process and thus
  achieve a significant community resource.
\end{task}

% Michael, I think triformal theories would be easier to start with findstat.org
% There are many reasons: more consistent structure in the mathematical data, more established research patterns, more consistent database storage, tighter integration of the code with sage code (in fact copy paste), etc

\begin{task}[title=FindStat Case Study (triformal theories),id=data-findstat,
  lead=JU,partners={ZH},PM=3,wphases=18-24!.5]
  In this task we would develop triformal theories for the \FindStat project to test the
  design from \localtaskref{data-foundationCAS}.  Similarly to the previous task, in this
  case study, we first develop a thorough OMDoc/MMT model, which should only involve a
  handful of MMT theories (combinatorial collections, maps, statistics,...), each with a
  few hundred realisations. Together with   \WPref{UI}, this will again allow for
  easier knowledge management services, and in particular improved search services.

  This Task will be co-developed with \localtaskref{data-foundationCAS}, it will validate
  the design of triformal theories and be iterated to test the design changes.
\end{task}

\begin{task}[title=\LMFDB Case study (triformal theories),id=data-LMFDB,
  lead=JU,partners={ZH,UW},PM=12,wphases=24-48!.5]
  In this task we would develop triformal theories for an exemplary part of the \LMFDB
  project to test the design from \localtaskref{data-foundationCAS}.  We will identify a
  fragment of the \LMFDB that we want to model and design the model. Then we will perform
  cross-validation of the three model parts against each other (essentially model-based
  testing of software and inference). Finally, we will pick an algorithm from the \LMFDB
  and verify it against its specification and the computational foundation developed in
  \localtaskref{data-foundationCAS}. 
  \end{task}

\begin{task}[title=Memoization and production of new data,id=data-memo,
  lead=SA,partners={US,PS,UW},PM=12,wphases=24-48!.5]
  Many CAS users run large and intensive computations, for which they want to collect the
  results while simultaneously working on software improvements. \GAP retains computed
  attribute values of objects within a session; \Sage currently has a limited
  \lstinline{cached_method}. Neither offers storage is not persistent across sessions or
  supports publication of the result or sharing within a collaboratoration. We will use,
  extend and contribute back to, an appropriate established persistent memoization
  infrastructure, such as \texttt{python-joblib}, \texttt{redis-simple-cache} or
  \texttt{dogpile.cache}, adding features needed for storage and use of results in
  mathematical research. We will design something that is simple to deploy and configure,
  and makes it easy to share results in a controlled manner, but provides enough assurance
  to enable the user to rely on the data, give proper credit to the original computation
  and rerun the computation if they with to.

%Mock code:
%    \begin{lstlisting}
%       mycloud = storage("ssh:xxx@yyy/zzz")
%       memoize(sage.combinat...., storage=mycloud, input=ZZ, output=Posets(), key="catalan")
%    \end{lstlisting}
\end{task}
\end{tasklist}

\begin{wpdelivs}
  \begin{wpdeliv}[due=9,id=wsrep,dissem=PU,nature=R,lead=JU]{\DKS base survey and
      Requirements Workshop Report}
  \end{wpdeliv}
  \begin{wpdeliv}[due=9,id=oeisparser,dissem=PU,nature=OTHER,lead=JU]
      {Heuristic Parser for the OEIS}
  \end{wpdeliv}
  \begin{wpdeliv}[due=12,id=dkstheories,dissem=PU,nature=R,lead=JU]
        {Design of Triform (DKS) Theories (Specification/RNC Schema/Examples)}
  \end{wpdeliv}
  \begin{wpdeliv}[due=12,id=pssyntax,dissem=PU,nature=DEC,lead=JU]
        {Python/Sage Syntax Foundation Module in OMDoc/MMT}
  \end{wpdeliv}
  \begin{wpdeliv}[due=12,id=conv,dissem=PU,nature=DEC,lead=ZH]
        {Conversion of existing and new databases to unified interoperable system}
   \end{wpdeliv}
  \begin{wpdeliv}[due=12,id=lmfmod,dissem=PU,nature=R,lead=ZH]
      {\LMFDB deep modelling: Fragment Identification \& Initial Model Design}
  \end{wpdeliv}
  \begin{wpdeliv}[due=18,id=lmfval,dissem=PU,nature=R,lead=ZH]
      {\LMFDB Data vs. Knowledge vs. Software Validation}
  \end{wpdeliv}
  \begin{wpdeliv}[due=18,id=oeisvalidation,dissem=PU,nature=R,lead=JU]
      {Cross-Validation for OEIS DKS-Theories}
  \end{wpdeliv}
  \begin{wpdeliv}[due=24,id=persistent-memoization,dissem=PU,nature=OTHER,lead=SA]
    {Shared persistent memoization library for Python/Sage} 
  \end{wpdeliv}
  \begin{wpdeliv}[due=24,id=dksimp,dissem=PU,nature=OTHER,lead=JU]
        {Implementation of Triform Theories in the MMT API}
  \end{wpdeliv}
  \begin{wpdeliv}[due=24,id=psfoundation,dissem=PU,nature=OTHER,lead=JU]
        {Python/Sage Computational Foundation Module in OMDoc/MMT}
  \end{wpdeliv}
  \begin{wpdeliv}[due=36,id=pssem,dissem=PU,nature=OTHER,lead=JU]
      {Python/Sage Declarative Semantics in OMDoc/MMT}
  \end{wpdeliv}
  \begin{wpdeliv}[due=36,id=lfmverif,dissem=PU,nature=OTHER,lead=JU]
      {\LMFDB algorithm verification with respect to a Triformal theory}
  \end{wpdeliv}
  \begin{wpdeliv}[due=48,id=lfmint,dissem=PU,nature=R,lead=JU]
      {\LMFDB integration of algorithms, data and presentation}
  \end{wpdeliv}
\end{wpdelivs}

\begin{comment}
Another connection: on several occasions, we found that software was the best way to
represent certain databases of mathematical knowledge. E.g. in Algebraic Combinatorics we
have a whole zoo of Hopf algebras. Many of them are implemented in MuPAD/Sage by
specifying the objects that index the basis together with computation rules for the
product and coproduct. When we want to retrieve information about such algebras, it's
usually much more convenient to look at the code than to search through the
literature. Especially since the code is usually more correct than the literature because
it's *tested*.

We may also think of providing an interface to \LMFDB via SCSCP
protocol (http://www.symbolic-computing.org/scscp) so it may
be accessed by a variety of other systems (see their current
list at http://www.symbolic-computing.org/scscp). But it's probably as
good to access it via \Sage.

\end{comment}
\end{workpackage}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../proposal"
%%% End:

%  LocalWords:  workpackage dksbases wphases wpobjectives standardise visualisation emph
%  LocalWords:  wpdescription Swinnerton-Dyer Millenium Borcherds optimised tasklist conv
%  LocalWords:  parallelizable maximise organise biform specialised trifunctional TOWRITE
%  LocalWords:  triformal findstat.org data-findstat localtaskref realisations texttt wrt
%  LocalWords:  Memoization python-joblib texttt redis-simple-cache texttt dogpile.cache
%  LocalWords:  lstlisting mycloud memoize sage.combinat wpdelivs wpdeliv dissem Polymake
%  LocalWords:  Recomputation wsrep dkstheories dksimp pssyntax psfoundation pssem lfmmod
%  LocalWords:  modelling lfmval lfmverif lfmint oeisparser oeisvalidation Hopf coproduct
%  LocalWords:  compactitem decentralised Logilab
