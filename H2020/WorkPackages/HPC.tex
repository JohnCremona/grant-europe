\begin{workpackage}[id=hpc,wphases=36-48,
  title=High Performance Computing,
  PSRM=1, % HPC for Combinatorics
  LLRM=12, % Pythran
  SARM=1, % GAP
  UKRM=1, % Singular
  UBRM=1, % Pari
  UJFRM=12] % Linbox / Pythran
  
\begin{wpobjectives}
  The objective of this work package is to improve the performance of
  the computational components of \TheProject, in particular on
  massively parallel architectures. This includes notably:
  \begin{itemize}
  \item Fine grained High Performance Computing on many-cores
    architectures.
  \item Coarse grained or embarrassingly parallel computing on grids
    or on the cloud.
  \item Compilation of high level interpreted code to optimized
    parallel native code.
  \item Develop novel HPC infrastructure in the context of
    combinatorics.
  \end{itemize}
  A key aspect will be to foster further sharing expertise and best
  practices between computational components.
\end{wpobjectives}

\begin{wpdescription}
  As in all other areas of science, properly supporting massively
  parallel architecture is a major challenge. Many of the
  computational components in \TheProject have already gone a long way
  in this direction. For example, an adaptation of the \GAP kernel for
  HPC was developed during the 2009-2013 EPSRC project. The expertise
  gained there was then transferred to the ongoing \Singular-HPC
  project, in particular through the rehiring of one of the developers
  of \GAP-HPC. The French ANR HPAC project (2012-2015) has also widely contributed to design
  parallel exact linear algebra kernels  which is a core component for most HPC
  applications. The \texttt{LinBox} library, used in sage, has benefited from this
  experience on the multi-core processing aspects. 

  In this work package, we will build on this momentum to further
  implement HPC support in the components Tasks~\ref{task:hpc_pari},
  \ref{task:hpc_linbox}, and \ref{task:hpc_singular}.

  \TODO{transition}

  Many of the computational components of \TheProject use a high level
  interpreted language for their library. This is notably the case of
  \Sage. Performance is achieved by compiling critical sections using
  the \Cython \Python-to-C compiler, to the expense of a lower level
  implementation. In Tasks~\ref{task:pythran_cython}
  and~\ref{task:pythran_sage}, we will also boost performance by
  further developing and applying such compilation tools, while
  keeping a high-level approach.

\end{wpdescription}
\begin{tasklist}
\begin{task}[title=PARI]
  \label{task:hpc_pari}
  \Pari is a C library mainly oriented toward arithmetic and number theory.
  
  It currently lacks interfaces for parallelism (POSIX threads or
  MPI). More precisely, it should be possible from an external package
  or software (e.g. Sage) to better exploit \Pari parallel features.

  On the other hand, many basic algorithms in PARI (e.g. integer
  factorization) are currently implemented using only one core. One can
  take better use of multi-core architecture and more generally parallel
  architectures.
\end{task}

\begin{task}[title=GAP]
  \label{task:hpc_gap}
  \TOWRITE{SL/AK}{Task around HPC/parallelism/perf in GAP}

  \TODO{deliverable}
\end{task}

\begin{task}[title=Linbox]
  \label{task:hpc_linbox}

  \begin{itemize}
  \item Large scale parallelization of core computations
    \begin{itemize}
    \item Higher levels algorithms
    \item Shared memory parallelization
    \end{itemize}
  \item Closer integration of existing parallel features in \Sage
    \begin{itemize}
    \item DSL, runtime
    \item resource management, etc
    \end{itemize}
  \end{itemize}
  \TOWRITE{JGD/CP}{Task around HPC/parallelism/perf in Linbox}

  \TODO{deliverable}
\end{task}

\begin{task}[title=Singular]
  \label{task:hpc_singular}
  The unique challenge of parallelizing Singular has been that it is a decades-old
  project, with a codebase exceeding 300,000 lines of code and an enormous existing
  investment of development effort. This makes a wholesale manual rewrite or reengineering
  approach infeasible.

  We therefore use a multi-pronged approach: First, we have created automated
  source-to-source translation tools that take existing C/C++ code as input and generate
  threadsafe code as output. Secondly we are also adding facilities to the C/C++ code and
  the Singular interpreter to safely access shared memory. These facilities ensure in
  particular that common pitfalls of concurrent programming, such as data races and
  deadlocks, cannot occur. For this, we leverage approaches that have already been
  successfully used for HPC-GAP and whose principles are well-understood.

  To supplement the above existing work, we propose to add very fine-grained parallelism
  to some key components of Singular. These include writing a multi-threaded
  implementation of the Singular multivariate polynomial arithmetic, of the main quadratic
  sieve implementation for integer factoring and parallelisation of the FFT based integer
  and dense polynomial multiplication algorithm. These key components are used extensively
  for Singular's overall workload, including in the Groebner basis engine and polynomial
  subsystems. Performance increases through fine-grained parallelisation of key components
  such as these will provide extensive benefits to virtually all users of Singular on
  multi-core machines.
\end{task}
\end{tasklist}

\begin{wpdelivs}
  \begin{wpdeliv}[due=3,id=del:QS_sieving,dissem=??,nature=??]
      {Parallelise the relation sieveing component of the Quadratic Sieve}
\end{wpdeliv}
  \begin{wpdeliv}[due=9,id=del:QS_linalg,dissem=??,nature=??]
      {Implement a parallel version of Block-Wiederman linear algebra over GF2 and the triple large prime variant.}
\end{wpdeliv}
  \begin{wpdeliv}[due=12,id=del:FFT,dissem=??,nature=??]
    {Take advantage of multiple cores in the matrix Fourier Algorithm component of the FFT
      for integer and polynomial arithmetic,and include assembly primitives for SIMD
      processor instructions (AVX, Knight's Bridge, etc.), especially in the FFT
      butterflies}
\end{wpdeliv}
  \begin{wpdeliv}[due=12,id=del:singular_polymul,dissem=??,nature=??]
      {Parallelise the Singular sparse polynomial multiplication algorithms}
\end{wpdeliv}
  \begin{wpdeliv}[due=12,id=del:singular_polyarith,dissem=??,nature=??]
      {Parallel versions of the Singular sparse polynomial division and GCD algorithms.}
\end{wpdeliv}
\end{wpdelivs}

\begin{tasklist}
\begin{task}{MPIR}
  \label{task:hpc_singular}
MPIR (Multiple Precision Integers and Rationals) is the core library in Sage
for bignum arithmetic. It is used extensively by a majority of the core C/C++
libraries in Sage, and by Sage directly via Cython. MPIR is a fork of the 
GMP (Gnu Multi-precision) library, with many independent implementations of the
core algorithms (including a faster FFT and division code, better 
superoptimisation on some common 64 bit processors and native MSVC support). 
It consists of around 250,000 lines of code, much of which is assembly 
primitives and very low level, highly optimised C code.

Maintainence of MPIR is not merely a matter of updating the build system.
Rather, every time a new processor is released by AMD, Intel, Sparc or ARM,
significant development has to be invested in hand-optimising and then
superoptimising assembly code for the new processors. This gives up to a 12x
performance increase over optimised C code, due to the specialised nature of
bignum arithmetic, which is in some sense a worst case for C compilers. Indeed
without continuous effort, MPIR would not even run on new operating systems and
processors, let alone run fast. This is a unique problem that assembly libraries
have.

As a successful and key component of Sage, we believe it is time to invest in
maintenance and improvement of MPIR by hiring an assembly expert who can work
full time on the project after MPIR's lead assembly expert sadly passed
away recently. Significant challenges exist, such as
optimising for SIMD instruction sets. Without investment into maintenance,
assembly superoptimisation, processor support, fat binary support, etc. this key
component of Sage will fall behind, to the detriment of Sage as a whole and the
numerous other standalone libraries that make use of MPIR.
\end{task}
\end{tasklist}

\begin{wpdelivs}
  \begin{wpdeliv}[due=12,id=del:MPIRsuperoptimiser,dissem=??,nature=??]
      {Extend the existing assembly superoptimiser for AVX and upcoming Intel processor extensions}
\end{wpdeliv}
  \begin{wpdeliv}[due=24,id=del:MPIRprocessors,dissem=??,nature=??]
      {Ongoing support of Intel, AMD, ARM, Sparc processors and new Operating System versions}
\end{wpdeliv}
  \begin{wpdeliv}[due=12,id=del:MPIRfat,dissem=??,nature=??]
      {Provide FAT binary support for all new x86\_64 processors, build system maintenance and improvements to tuning, profiling and testing subsystems}
\end{wpdeliv}
\end{wpdelivs}


\begin{tasklist}
\begin{task}[title=HPC infrastructure for combinatorics,id=hpc _combi]
  \TOWRITE{FH}{Task around HPC infrastructure for combinatorics}

  Several members of the projects are experts in combinatorics a field where
  Sage is clearly a world leader. This particular research field has several
  specific features which makes it interesting from the HPC point of view:
  \begin{itemize}
  \item The main obstacle is combinatorial explosion which stop many
    experiment very early. Algorithm of exponential complexity are extremely
    common.
  \item Embarrassingly parallel problem are extremely common.
  \item Problem that are not embarrassingly parallel very often reduce to a
    large tree exploration.
  \end{itemize}
  To help the researcher, it is crucial to minimalize the extra work needed to
  get a parallel program from a serial one in these simple situations. Through
  this task we will provide a practical and concrete as well as highly
  demanding use case for the infrastructure developed. In particular, we aim
  to test on real problems the benefit of tasks~\ref{task:pythran_cython},
  \ref{task:mod_packaging}, \ref{task:deployment_distrib},
  and \ref{task:component_for_HPC}.

  In a second and more exploratory direction, some experiments shows that the
  large tree exploration problem is very easily solved in C++ using the new
  Intel \texttt{Cilk++} technology. We would like to explore the possibility to
  interface smoothly \Pythran, \Cython and \texttt{Cilk++}.
\end{task}
\end{tasklist}

\begin{wpdelivs}
  \begin{wpdeliv}[due=3,id=del:HPCcombi,dissem=??,nature=??]
      {Turn the Python prototypes for tree exploration into    production code, integrate to \Sage.}
\end{wpdeliv}
  \begin{wpdeliv}[due=12,id=del:HPCcombi,dissem=??,nature=??]
      {Explore the possibility to    interface smoothly \Pythran, \Cython and \texttt{Cilk++}.}
\end{wpdeliv}
  \begin{wpdeliv}[due=24,id=del:HPCcombi,dissem=??,nature=??]
      {Refactor the existing combinatorics \Sage    code using the new developed  \Pythran and \Cython features.}
\end{wpdeliv}
\end{wpdelivs}

\begin{tasklist}
\begin{task}[title=Pythran-Cython convergence]
  \label{task:pythran_cython}

  \Pythran is a \Python to C++ compiler for a subset of the \Python
  language. It is meant to efficiently compile scientific programs,
  and takes advantage of multi-cores and SIMD instruction units.
  Thanks to type inference, it requires little annotations. Its rutime
  supports a subset of the \Numpy package.

  \Cython is a \Python to C compiler that was originally developed for
  \Sage and is now a thriving project of its own. It can handle
  essentially any \Python code, and in particular classes, but relies
  heavily on annotations for producing optimized code.

  Therefore, \Pythran and \Cython are similar in spirit but have
  complementary feature sets: \Pythran can heavily optimize high level
  \Numpy constructs and \Cython has broader \Python support. In this
  task, we will investigate the opportunity and feasibility of a
  convergence between \Cython and \Pythran: depending on the code at
  hand, one strategy or the other would be automatically selected,
  eventually using \Pythran generated called from \Cython when
  relevant D~\ref{del:pythran_cython}. This would result in compiler-runtime
  cooperation driven by the \Cython compiler thanks to part of the
  \Pythran-runtime and the extra typing information provided by \Cython. An
  effort will be made to improve more and more the parallelism in the
  \Pythran runtime D~\ref{del:pythran_runtime}.

  This work will be achieved through a close collaboration between the \Pythran
  developers hired for \TheProject and \Cython developers involved in the \Sage
  project. It should quicken \Sage execution time at least on \Numpy centric
  codes, while not putting an extra burden on the developers.  Preleminary
  dicussions with the \Cython community have already taken place and received a
  very favorable feedback.
\end{task}

\begin{task}[title=\Pythran for \Sage and \Sage Users]
  \label{task:pythran_sage}

  Currently, \Sage doesn't provide facilities to improve user written
  \Python code without the modifications implied by the use of the \Cython
  compiler. As \Pythran doesn't need these codes to be rewriten, a notebook
  interface to compile \Pythran compliant code will he added in \Sage to
  improve user kernels using the \Pythran compiler D~\ref{del:pythran_sage}.

  In a similar perspective, testing and improving the integeration between
  \software{mpi4py} and \Pythran could provide an efficient toolchain for HPC
  while keeping full backward compatibility with pure \Python code. This will
  required a continuous integration of \Pythran to ensure its capabilities
  D~\ref{del:pythran}.

  Internally, \Sage uses \Cython for compiling the critical sections of
  its libraries. In this task, we will explore opportunities to
  benefit from \Pythran compilation within the \Sage library, in
  particular toward better support for parallelism. A specific
  challenge is that the \Sage library uses quite heavily
  object-oriented programming.

  This task will strongly benefit from Task~\ref{task:pythran_cython},
  while providing in return a real life large-scale use case for it.

  A first step to support object-oriented programming will be to make
  \Pythran type inference more accurate, which will also improve error
  feedback provided for the user D~\ref{del:pythran_typing}.
\end{task}

\begin{task}[title=Explorative task: Add support for classes in \Pythran.]
  Classes support is a real challenge for \Pythran as it requires a more
  accurate typing information but also invalidates some compiler optimisations.

  As it will need a full rework of the aliases analysis in \Pythran, which is
  the keystone of \Pythran, we are not sure it could really
  be integrated but it would be a proof of scalability for \Pythran.
  Thanks to this typing and this better aliaing analysis, we could add more
  optimizations like the ones from \Cython enabled with decorator annotation.
\end{task}
\end{tasklist}

\begin{wpdelivs}
  \begin{wpdeliv}[due=6,id=del:pythran_cython,dissem=??,nature=??]
      {Implement \Pythran runtime support in \Cython when they are implemented instead of using default implementation.}
\end{wpdeliv}
  \begin{wpdeliv}[due=3,id=del:pythran_runtime,dissem=??,nature=??]
      {Improve \Pythran runtime support to automatically take advantage of multi-cores and SIMD instruction units.}
\end{wpdeliv}
  \begin{wpdeliv}[due=2,id=del:pythran_sage,dissem=??,nature=??]
      {Facility to compile \Pythran compliant user kernels.}
\end{wpdeliv}
  \begin{wpdeliv}[due=1,id=del:pythran,dissem=??,nature=??]
      {Ensure inteoperability of \Pythran with \Python and its packages.}
\end{wpdeliv}
  \begin{wpdeliv}[due=12,id=del:pythran_typing,dissem=??,nature=??]
      {Make \Pythran typing better to improve error information.}
\end{wpdeliv}
\end{wpdelivs}
\end{workpackage}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../proposal"
%%% End:
